{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10469451,"sourceType":"datasetVersion","datasetId":6482368},{"sourceId":341771,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":285883,"modelId":306722},{"sourceId":342892,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":286818,"modelId":307628}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\ndataset_path = '/kaggle/input/seismic-dataset/'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"import h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport random\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor\nfrom torchsummary import summary\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self, data_path):\n        super().__init__()\n        self.path = data_path\n        self.high_res_path = os.path.join(self.path, 'high')\n        self.low_res_path = os.path.join(self.path, 'low')\n        self.high_res = [x for x in os.listdir(self.high_res_path)]\n        self.low_res = [x for x in os.listdir(self.low_res_path)]\n        self.low_res_file_path = [os.path.join(self.low_res_path, x) for x in self.low_res]\n        self.high_res_file_path = [os.path.join(self.high_res_path, x) for x in self.high_res] \n\n    def __getitem__(self, index):\n        high_h5_data = self._read_h5(self.high_res_file_path[index])\n        low_h5_data = self._read_h5(self.low_res_file_path[index])\n\n        low_h5_data,high_h5_data = self._agumentation(low_h5_data,high_h5_data)\n\n        low_h5_data = self._normalize_image(low_h5_data)\n        high_h5_data = self._normalize_image(high_h5_data)\n        \n        return (torch.tensor(low_h5_data, dtype=torch.float32).unsqueeze(0),\n                torch.tensor(high_h5_data, dtype=torch.float32).unsqueeze(0))\n\n    def __len__(self):\n        return len(self.low_res_file_path)      \n\n    def _agumentation(self,low_data,high_data):\n        if random.random() > 0.5:\n            high_data = np.flip(high_data, axis=0)\n            low_data = np.flip(low_data, axis=0)\n        if random.random() > 0.5:\n            high_data = np.flip(high_data, axis=1)\n            low_data = np.flip(low_data, axis=1)\n        if random.random() > 0.5:\n            high_data = np.rot90(high_data, k=2)\n            low_data = np.rot90(low_data, k=2)\n        return low_data,high_data\n    \n    @staticmethod\n    def _read_h5(file_path):\n        import h5py\n        with h5py.File(file_path, 'r') as f:\n            return f['data'][:]\n\n    @staticmethod\n    def _normalize_image(image):\n    # Assuming image is a NumPy array or torch tensor\n        min_val = image.min()\n        max_val = image.max()\n        normalized_image = 2 * ((image - min_val) / (max_val - min_val)) - 1\n        return normalized_image\n            \n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset=dataset(os.path.join(dataset_path,'SRF_2/train'))\nval_dataset=dataset(os.path.join(dataset_path,'SRF_2/val'))\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"low,high=train_dataset[200]\nprint(low.mean())\nprint(low.max(),low.min())\nprint(high.max(),high.min())\nprint(low.shape,high.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_loader))\nprint(len(train_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_features, train_labels = next(iter(train_loader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pytorch_msssim","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation Metrices","metadata":{}},{"cell_type":"code","source":"\nfrom pytorch_msssim import ssim as msssim\n\n# PSNR function (with proper handling)\n\ndef norm(image):\n    max=image.max()\n    min=image.min()\n    normal = (image - min) / (max-min)\n    return normal\n   \ndef psnr(img1, img2):\n    img1=norm(img1)\n    img2=norm(img2)\n    mse = F.mse_loss(img1, img2)\n    if mse == 0:\n        return 100  # If no error, PSNR is infinite\n    return 10 * torch.log10(1 / mse)\n\ndef ssim(img1, img2):\n    img1=norm(img1)\n    img2=norm(img2)\n    return msssim(img1, img2, data_range=1.0, size_average=True).item()\n\ndef frequency_distance(img1, img2):\n    img1=norm(img1)\n    img2=norm(img2)\n    # Perform FFT on both images\n    fft1 = torch.fft.fft2(img1.squeeze(1))  # img1 should have shape [batch_size, 1, H, W]\n    fft2 = torch.fft.fft2(img2.squeeze(1))  # img2 should have shape [batch_size, 1, H, W]\n    \n    # Compute the magnitude of the FFT (absolute value of the complex numbers)\n    mag1 = torch.abs(fft1)\n    mag2 = torch.abs(fft2)\n    \n    # Compute the L2 distance between the magnitude spectra\n    return F.mse_loss(mag1, mag2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loss Functions","metadata":{}},{"cell_type":"code","source":"## Wasserstein Loss\ndef wasserstein_loss(real_preds, fake_preds):\n    return -(torch.mean(real_preds) - torch.mean(fake_preds))\n\n## Gradient Penalty\ndef gradient_penalty(critic, real_samples, fake_samples):\n    alph = torch.rand(real_samples.size(0), 1, 1, 1, device=real_samples.device)\n    interpolates = (alph * real_samples + (1 - alph) * fake_samples).requires_grad_(True)\n    d_interpolates = critic(interpolates)\n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=torch.ones(d_interpolates.size(), device=real_samples.device),\n        create_graph=True,\n        retain_graph=True\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_norm = gradients.norm(2, dim=1)\n    penalty = torch.mean((gradient_norm - 1) ** 2)\n    return penalty\n\n# TV Loss\ndef total_variation_loss(img):\n    diff_x = img[:, :, 1:, :] - img[:, :, :-1, :]\n    diff_y = img[:, :, :, 1:] - img[:, :, :, :-1]\n    return torch.sum(torch.abs(diff_x)) + torch.sum(torch.abs(diff_y))\n\n# Loss functions\nclass VGGLoss(nn.Module):\n    def __init__(self):\n        super(VGGLoss, self).__init__()\n        self.vgg = torchvision.models.vgg16(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(self.vgg.features)[:31]).eval()\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n\n    def forward(self, pred, target):\n        pred_features = self.feature_extractor(pred)\n        target_features = self.feature_extractor(target)\n        return F.mse_loss(pred_features, target_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directory to save models\noutput_dir = \"/kaggle/working/models\"\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Tensorboard Logging","metadata":{}},{"cell_type":"code","source":"import os\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Define your main logging directory and subdirectories for train and validation\ntensorboard_dir = \"/kaggle/working/tensorboard_logs\"\n\n# Create main directory if it doesn't exist\nif not os.path.exists(tensorboard_dir):\n    os.mkdir(tensorboard_dir)\n\n# Create subdirectories for train and validation\ntrain_log_dir = os.path.join(tensorboard_dir, 'train')\nval_log_dir = os.path.join(tensorboard_dir, 'validation')\n\nif not os.path.exists(train_log_dir):\n    os.mkdir(train_log_dir)\nif not os.path.exists(val_log_dir):\n    os.mkdir(val_log_dir)\n\n# Create TensorBoard SummaryWriter for training and validation logs\ntrain_writer = SummaryWriter(log_dir=train_log_dir)\nval_writer = SummaryWriter(log_dir=val_log_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Converting all output to single zip file","metadata":{}},{"cell_type":"code","source":"import zipfile\ndef save_and_compress_outputs(dirs_to_save, zip_filename):\n    zip_path = os.path.join(\"/kaggle/working\", zip_filename)\n    with zipfile.ZipFile(zip_path, 'w') as zf:\n        for save_dir in dirs_to_save:\n            for root, _, files in os.walk(save_dir):\n                for file in files:\n                    zf.write(\n                        os.path.join(root, file),\n                        arcname=os.path.relpath(os.path.join(root, file), save_dir),\n                    )\n    print(f\"Outputs compressed and saved to {zip_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Generator","metadata":{}},{"cell_type":"code","source":"# Define the Residual Block for the Generator\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.prelu = nn.PReLU()\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.bn1(residual)\n        residual = self.prelu(residual)\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n        return x + residual\n\n# Define the Generator\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=9, stride=1, padding=4)\n        self.prelu = nn.PReLU()\n        \n        # Residual blocks\n        self.residual_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(12)])\n        \n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        # Upsampling blocks\n        self.upsample1 = nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1)\n       \n        self.pixel_shuffle = nn.PixelShuffle(2)\n    \n        self.conv3 = nn.Conv2d(64, 1, kernel_size=9, stride=1, padding=4)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.prelu(x)\n        residual = x\n        x = self.residual_blocks(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = x + residual\n        x = self.prelu(x)\n        x = self.upsample1(x)\n        x = self.pixel_shuffle(x)\n        x = self.prelu(x)\n        x = self.conv3(x)\n        \n        return self.tanh(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Discriminator","metadata":{}},{"cell_type":"code","source":"# Discriminator Model (PatchGAN)\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_channels, out_channels, stride):\n            block = [\n                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.LeakyReLU(0.2, inplace=True)\n            ]\n            return block\n\n        # PatchGAN structure\n        self.model = nn.Sequential(\n            *discriminator_block(1, 64, 1),\n            *discriminator_block(64, 64, 2),\n            *discriminator_block(64, 128, 1),\n            *discriminator_block(128, 128, 2),\n            *discriminator_block(128, 256, 1),\n            *discriminator_block(256, 256, 2),\n            *discriminator_block(256, 512, 1),\n            *discriminator_block(512, 512, 2),\n        )\n        \n        # Final layers\n        self.pool = nn.AdaptiveAvgPool2d(2)\n        self.fc1 = nn.Conv2d(512,1024, kernel_size=1)\n        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n        self.fc2 = nn.Conv2d(1024,1, kernel_size=1)\n        \n    def forward(self, x):\n        x = self.model(x)\n        x = self.pool(x)\n        x = self.fc1(x)\n        x = self.leaky_relu(x)\n        x = self.fc2(x)\n        return x  # Output will be a matrix of size [batch_size, 1, H/16, W/16]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Checkpoint Loading","metadata":{}},{"cell_type":"code","source":"#For training\nmse_loss = nn.MSELoss()\nvgg_loss = VGGLoss().cuda()\n\ncheckpoint_path = '/kaggle/input/epoch120/pytorch/default/1/checkpoint_epoch_120.pth'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initialization","metadata":{}},{"cell_type":"code","source":"epochs = 200\nalpha = 2e-2\nbeta = 6e-2\nlambda_gp = 12  #Gradient penalty coefficient\ncritic_steps = 1\nlamda=2e-7\n\n\n#Initialize models\ngenerator = Generator().cuda()\ndiscriminator = Discriminator().cuda()\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), \n                 lr=3e-5,              # Learning rate\n                 betas=(0.9, 0.999),   # Momentum parameters\n                 weight_decay=1e-5)    # L2 regularization\n\n# Discriminator Optimizer\noptimizer_D = torch.optim.Adam(discriminator.parameters(),\n                 lr=1e-4,              # Learning rate\n                 betas=(0.9, 0.999),   # Momentum parameters\n                 weight_decay=1e-5)    # L2 regularization\n\n\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    generator.load_state_dict(checkpoint['generator_state_dict'])\n    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n\n    start_epoch = checkpoint['epoch']  # Resume from last saved epoch\n    print(f\"Resuming training from epoch {start_epoch}\")\n\nelse:\n    print(\"No checkpoint found. Training from scratch.\")\n    start_epoch = 0  # Start from the beginning","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"# Training loop\nfor epoch in range(start_epoch,epochs):\n    generator.train()\n    discriminator.train()\n    \n    for i, (low_res, high_res) in enumerate(train_loader):\n        low_res, high_res = low_res.cuda(), high_res.cuda()\n\n        # Train Discriminator\n        for _ in range (critic_steps):\n            optimizer_D.zero_grad()\n            fake_high_img=generator(low_res).detach()\n            real_pred=discriminator(high_res)\n            fake_pred=discriminator(fake_high_img)\n            \n            gp=gradient_penalty(discriminator, high_res, fake_high_img)\n            d_loss = wasserstein_loss(real_pred,fake_pred) + lambda_gp * gp\n            d_loss.backward()\n            optimizer_D.step()\n\n        # Train Generator\n        optimizer_G.zero_grad()\n        gen_high_res = generator(low_res)\n        gen_high_res_3channel=gen_high_res.repeat(1,3,1,1)\n        high_res_3channel=high_res.repeat(1,3,1,1)\n        \n        g_loss_vgg = vgg_loss(gen_high_res_3channel, high_res_3channel) #Perseptual Loss\n        \n        g_loss_mse = mse_loss(gen_high_res, high_res)\n        g_loss_TV = total_variation_loss(gen_high_res)\n        gen_pred=discriminator(gen_high_res)\n        g_loss_adv = -torch.mean(gen_pred) #Perseptual Loss\n        g_loss = g_loss_mse + beta * g_loss_vgg + alpha * g_loss_adv + lamda*g_loss_TV\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n        g_loss.backward()\n        optimizer_G.step()\n\n         # Log losses to TensorBoard\n        pred = generator(low_res).detach()\n        # Compute Metrics\n        psnr_value = psnr(pred, high_res)\n        ssim_value = ssim(pred, high_res)\n\n        # Log losses to TensorBoard\n        train_writer.add_scalar(\"PSNR/train\", psnr_value.item(), epoch * len(train_loader) + i)\n        train_writer.add_scalar(\"SSIM/train\", ssim_value, epoch * len(train_loader) + i)\n        train_writer.add_scalar('Loss/Discriminator', d_loss.item(), epoch * len(train_loader) + i)\n        train_writer.add_scalar('Loss/Generator', g_loss.item(), epoch * len(train_loader) + i)\n        train_writer.add_scalar('Loss/MSE', g_loss_mse.item(), epoch * len(train_loader) + i)\n        train_writer.add_scalar('Loss/VGG', g_loss_vgg.item(), epoch * len(train_loader) + i)\n        train_writer.add_scalar('Loss/Adversarial', g_loss_adv.item(), epoch * len(train_loader) + i)\n\n        if i % 10 == 0:\n            print(f\"Epoch [{epoch}/{epochs}] Batch {i}/{len(train_loader)} \"\n                  f\"Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}, \"\n                  f\"MSE: {g_loss_mse.item():.4f}, VGG: {g_loss_vgg.item():.4f}, Adv: {g_loss_adv.item():.4f},PSNR:{psnr_value.item():.4f},SSIM:{ssim_value:.4f}\")\n\n    \n    # Validation phase\n    generator.eval()\n    discriminator.eval()\n    val_mse_loss, val_psnr,val_fft,val_ssim= 0.0, 0.0,0.0,0.0\n    with torch.no_grad():\n        val_g_loss=0.0\n        val_d_loss=0.0\n        for low_res, high_res in val_loader:\n            low_res, high_res = low_res.cuda(), high_res.cuda()\n            \n            gen_high_res = generator(low_res)\n            real_pred=discriminator(high_res)\n            fake_pred=discriminator(gen_high_res)\n            val_d_loss = wasserstein_loss(real_pred,fake_pred)\n            \n            gen_high_res_3channel=gen_high_res.repeat(1,3,1,1)\n            high_res_3channel=high_res.repeat(1,3,1,1)\n        \n            val_g_loss_vgg = vgg_loss(gen_high_res_3channel, high_res_3channel) #Perseptual Loss\n        \n            val_g_loss_mse = mse_loss(gen_high_res, high_res)\n            val_g_loss_TV = total_variation_loss(gen_high_res)\n            val_gen_pred= discriminator(gen_high_res)\n            val_g_loss_adv = -torch.mean(val_gen_pred) \n            val_g_loss =  val_g_loss_mse + beta * val_g_loss_vgg + alpha * val_g_loss_adv + lamda*val_g_loss_TV\n\n            val_writer.add_scalar('VAL/Discriminator', val_d_loss.item(), epoch * len(train_loader) + i)\n            val_writer.add_scalar('VAL/Generator', val_g_loss.item(), epoch * len(train_loader) + i)\n            val_writer.add_scalar('VAL/VGG',val_g_loss_vgg.item(), epoch * len(train_loader) + i)\n            val_writer.add_scalar('VAL/Adversarial', val_g_loss_adv.item(), epoch * len(train_loader) + i)\n            \n            val_mse_loss += val_g_loss_mse.item()\n            val_psnr += psnr(gen_high_res, high_res).item()\n            val_ssim += ssim(gen_high_res,high_res)\n            val_fft  += frequency_distance(gen_high_res,high_res).item()\n    val_mse_loss /= len(val_loader)\n    val_psnr /= len(val_loader)\n    val_ssim /= len(val_loader)\n    val_fft /= len(val_loader)\n    # Log validation metrics to TensorBoard\n    val_writer.add_scalar('Validation/MSE', val_mse_loss, epoch)\n    val_writer.add_scalar('Validation/PSNR', val_psnr, epoch)\n    val_writer.add_scalar('Validation/ssim',val_ssim,epoch)\n    val_writer.add_scalar('Validation/fft',val_fft,epoch)\n    print(f\"Validation - Epoch [{epoch}/{epochs}]: MSE: {val_mse_loss:.4f}, PSNR: {val_psnr:.4f}, SSIM: {val_ssim:.4f},FFT :{val_fft:.4f}\")\n\n    # Save the model every 10 epochs\n    if (epoch + 1) % 10 == 0:\n            torch.save({\n                'epoch': epoch + 1,\n                'generator_state_dict': generator.state_dict(),\n                'discriminator_state_dict': discriminator.state_dict(),\n                'optimizer_G_state_dict': optimizer_G.state_dict(),\n                'optimizer_D_state_dict': optimizer_D.state_dict(),\n            },os.path.join(output_dir, f\"checkpoint_epoch_{epoch+1}.pth\"))  \n            print(f\"Models saved at epoch {epoch+1}\")\nsave_and_compress_outputs([output_dir, tensorboard_dir], \"trained_models and logs.zip\")\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}